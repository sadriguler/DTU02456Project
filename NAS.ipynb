{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e82e669-e2ed-4e9d-8176-bbf84e806ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fa1dd3a-4511-4a10-ae78-fafc4c1faed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = MNIST(\"C:\\\\Users\\\\sadri\\\\Documents\\\\deepLearningProject\\\\\", train=True, download=True)\n",
    "mnist_testset = MNIST(\"C:\\\\Users\\\\sadri\\\\Documents\\\\deepLearningProject\\\\\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd1e7e0-dd2c-445d-ab3a-2b817ff1d979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information on dataset\n",
      "x_train torch.Size([1000, 1, 28, 28])\n",
      "targets_train torch.Size([1000])\n",
      "x_valid torch.Size([500, 1, 28, 28])\n",
      "targets_valid torch.Size([500])\n",
      "x_test torch.Size([500, 1, 28, 28])\n",
      "targets_test torch.Size([500])\n"
     ]
    }
   ],
   "source": [
    "# To speed up training we'll only work on a subset of the data\n",
    "x_train = mnist_trainset.data[:1000].view(-1, 1, 28, 28).float()\n",
    "targets_train = mnist_trainset.targets[:1000]\n",
    "\n",
    "x_valid = mnist_trainset.data[1000:1500].view(-1, 1, 28, 28).float()\n",
    "targets_valid = mnist_trainset.targets[1000:1500]\n",
    "\n",
    "x_test = mnist_testset.data[:500].view(-1, 1, 28, 28).float()\n",
    "targets_test = mnist_testset.targets[:500]\n",
    "\n",
    "print(\"Information on dataset\")\n",
    "print(\"x_train\", x_train.shape)\n",
    "print(\"targets_train\", targets_train.shape)\n",
    "print(\"x_valid\", x_valid.shape)\n",
    "print(\"targets_valid\", targets_valid.shape)\n",
    "print(\"x_test\", x_test.shape)\n",
    "print(\"targets_test\", targets_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc4ae9a3-f57d-4c84-931d-373375ea2cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "example_data = mnist_trainset.data[:6].view(-1,1,28,28)\n",
    "example_targets = targets_train[:6]\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97227685-13ea-48c3-a8ad-fc848938dcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('cuda is available!')\n",
    "else:\n",
    "    print('sorry, but no cuda found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "161364f2-f7bf-4604-b7f3-fd36cdb55b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listofconvnets(input_channels, num_channels, strides, nuconv,\n",
    "                 trans_block = False):\n",
    "    # nuconv: number of potential convolutional blocks, 1x1, 3x3, ...\n",
    "    # kernel size: filter dimensions\n",
    "    # stride: step size\n",
    "    # padding: filling by sides\n",
    "    blk = []\n",
    "    if trans_block:\n",
    "        for i in range(nuconv):\n",
    "            blk.append(nn.Conv2d(input_channels, num_channels,\n",
    "                            kernel_size=2*i+1, padding=i, stride=1))    \n",
    "                            # kernel_size=2*i+1, padding=i, stride=strides))\n",
    "            blk.append(nn.BatchNorm2d(num_channels))\n",
    "        for i in range(nuconv+1):\n",
    "            blk.append(nn.ConvTranspose2d(input_channels, num_channels,\n",
    "                                kernel_size=2*i+1, padding=i, stride=1))\n",
    "                            # kernel_size=2*i+1, padding=i, stride=strides))\n",
    "            blk.append(nn.BatchNorm2d(num_channels))\n",
    "    else:\n",
    "        for i in range(nuconv):\n",
    "            blk.append(nn.Conv2d(input_channels, num_channels,\n",
    "                            kernel_size=2*i+1, padding=i, stride=strides))\n",
    "            blk.append(nn.BatchNorm2d(num_channels))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0e87ef9-0ae6-4795-8087-5d91b33d70c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "thesoft = nn.Softmax(dim=0)\n",
    "class superarch(): \n",
    "    # class to define super-architecture, as in the main paper\n",
    "    # it switches on/off identity layers and creates a total of\n",
    "    # input*2 layers\n",
    "    def __init__(self,numoflayers, nuconvnet):\n",
    "        # nuconvnet: number of candidate convolutional net\n",
    "        self.arch = []\n",
    "        self.b = [] # hyperparameters for network selection, probabilities\n",
    "        for i in range(numoflayers):\n",
    "            for j in [False, True]:\n",
    "                layerelms = listofconvnets(64,64,2,nuconvnet,j) # possible networks are added in here\n",
    "                self.arch.append(layerelms)\n",
    "                if j:\n",
    "                    self.b.append(torch.rand(2*nuconvnet+1+1))\n",
    "                else:\n",
    "                    self.b.append(torch.rand(nuconvnet+1))\n",
    "        self.b0 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1), # kernel_size, ... :: 7, 2, 3\n",
    "                           nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                           nn.MaxPool2d(kernel_size=3, stride=1, padding=1))\n",
    "        self.bend = nn.Sequential(nn.AdaptiveAvgPool2d((1,1)),\n",
    "                            nn.Flatten(), nn.Linear(64, 10))\n",
    "        self.layersels = [] # current selected layers\n",
    "        self.thecalcnets = [] # all calculated networks until now\n",
    "        self.Iconv = [] # identity convolutional layers to decrease the data size\n",
    "        for i in range(numoflayers):\n",
    "            self.Iconv.append(nn.Conv2d(64, 64,kernel_size=1, stride=2))\n",
    "            \n",
    "    def tonetselect(self): # with given probabilities, find the candidate network\n",
    "        blk = [] \n",
    "        blk.append(self.b0)\n",
    "        self.layersels = []\n",
    "        for ind in range(superarchlen):\n",
    "            i =2*ind\n",
    "            loc = torch.argmax(self.b[i][:])\n",
    "            self.layersels.append(loc)\n",
    "            if (loc != 0):\n",
    "                # loc -= 1 ### edited by using (loc-1)\n",
    "                theconv = self.arch[i][2*(loc-1)]\n",
    "                thebatc = self.arch[i][2*(loc-1)+1]\n",
    "                blk.append(nn.Sequential(singleLayer(theconv,thebatc,self.Iconv[ind])))\n",
    "            else:\n",
    "                blk.append(nn.Sequential(self.Iconv[ind]))\n",
    "                \n",
    "            i = 2*ind+1\n",
    "            loc = torch.argmax(self.b[i][:])\n",
    "            self.layersels.append(loc)\n",
    "            if (loc != 0):\n",
    "                # loc -= 1 ### edited by using (loc-1)\n",
    "                theconv = self.arch[i][2*(loc-1)]\n",
    "                thebatc = self.arch[i][2*(loc-1)+1]\n",
    "                blk.append(nn.Sequential(singleLayer(theconv,thebatc,nn.Identity())))\n",
    "            else:\n",
    "                blk.append(nn.Sequential(nn.Identity()))\n",
    "        blk.append(self.bend)\n",
    "        net = nn.Sequential(*blk)\n",
    "        return net\n",
    "    \n",
    "    def checknetexistence(self): # check if the current selected network used before\n",
    "        for count, net in enumerate(self.thecalcnets):\n",
    "            if net == self.layersels:\n",
    "                return True\n",
    "        self.thecalcnets.append(self.layersels)\n",
    "        \n",
    "    def thenettosuperarch(self, net): # save the optimized layer parameters\n",
    "        self.bo = net[0]\n",
    "        for ind in range(superarchlen):\n",
    "            i = 2*ind\n",
    "            loc = self.layersels[i]\n",
    "            if (loc != 0):\n",
    "                print('ehehe')\n",
    "                self.arch[i][2*(loc-1)] = net[i+1][0]\n",
    "                self.arch[i][2*(loc-1)+1] = net[i+1][1]\n",
    "                self.Iconv[ind] = net[i+1][2]\n",
    "            else:\n",
    "                self.Iconv[ind] = net[i+1]\n",
    "                \n",
    "            i = 2*ind + 1\n",
    "            loc = self.layersels[i+1]\n",
    "            if (loc != 0):\n",
    "                self.arch[i][2*(loc-1)] = net[i+1][0]\n",
    "                self.arch[i][2*(loc-1)+1] = net[i+1][1]\n",
    "                self.Iconv[ind] = net[i+1][2]\n",
    "            else:\n",
    "                self.Iconv[ind] = net[i+1]\n",
    "                \n",
    "        self.bend = net[2*superarchlen+1]\n",
    "    \n",
    "    def printarch(self): # print all architecture layers\n",
    "        for layer in self.arch:\n",
    "            print(\"number of candidates: \",len(layer))\n",
    "            for i in range(int(len(layer)/2)):\n",
    "                print(i, layer[i*2].__class__.__name__)\n",
    "\n",
    "            #for count, singlenet in enumerate(layer):\n",
    "            #    print(count, singlenet)\n",
    "            print(\" \")\n",
    "        \n",
    "class singleLayer(nn.Module):  #@save\n",
    "    # The optimizable single layer of super-architecture \n",
    "    def __init__(self, conv, bn, iden):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv\n",
    "        self.bn1 = bn\n",
    "        self.iden = iden\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn1(self.conv1(X))\n",
    "        Y += self.iden(X)\n",
    "        return F.relu(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ce54f91f-b700-442e-b94c-319188c758cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "superarchlen = 5 # in reality, it is superarchlen*2\n",
    "initmysuperarch = superarch(superarchlen,3) # ..., number of candidate convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c0a00755-ff0d-4d81-b325-e3c57babf69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(3), tensor(4), tensor(0), tensor(6), tensor(0), tensor(0), tensor(3), tensor(0), tensor(2), tensor(5)]\n",
      "---------------------------|------------------\n",
      "None\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): singleLayer(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (iden): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "    )\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): singleLayer(\n",
      "      (conv1): ConvTranspose2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (iden): Identity()\n",
      "    )\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "  )\n",
      "  (4): Sequential(\n",
      "    (0): singleLayer(\n",
      "      (conv1): ConvTranspose2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (iden): Identity()\n",
      "    )\n",
      "  )\n",
      "  (5): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "  )\n",
      "  (6): Sequential(\n",
      "    (0): Identity()\n",
      "  )\n",
      "  (7): Sequential(\n",
      "    (0): singleLayer(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (iden): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "    )\n",
      "  )\n",
      "  (8): Sequential(\n",
      "    (0): Identity()\n",
      "  )\n",
      "  (9): Sequential(\n",
      "    (0): singleLayer(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (iden): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "    )\n",
      "  )\n",
      "  (10): Sequential(\n",
      "    (0): singleLayer(\n",
      "      (conv1): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (iden): Identity()\n",
      "    )\n",
      "  )\n",
      "  (11): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "10\n",
      "11\n",
      "ehehe\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2292/1825085366.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitmysuperarch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthecalcnets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msuperarchlen\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0minitmysuperarch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthenettosuperarch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthenet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2292/2030164462.py\u001b[0m in \u001b[0;36mthenettosuperarch\u001b[1;34m(self, net)\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ehehe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0march\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0march\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIconv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_by_idx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36m_get_item_by_idx\u001b[1;34m(self, iterator, idx)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'index {} is out of range'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[0midx\u001b[0m \u001b[1;33m%=\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mislice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of range"
     ]
    }
   ],
   "source": [
    "thenet = initmysuperarch.tonetselect()\n",
    "print(initmysuperarch.layersels)\n",
    "print('---------------------------|------------------')\n",
    "\n",
    "checkval = initmysuperarch.checknetexistence()\n",
    "print(checkval)\n",
    "\n",
    "print(thenet)\n",
    "print(len(initmysuperarch.thecalcnets[0]))\n",
    "print(2*superarchlen+1)\n",
    "initmysuperarch.thenettosuperarch(thenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7d616b77-af3e-441d-a2b7-f79de064c5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([32, 64, 28, 28])\n",
      "Sequential output shape:\t torch.Size([32, 64, 14, 14])\n",
      "Sequential output shape:\t torch.Size([32, 64, 14, 14])\n",
      "Sequential output shape:\t torch.Size([32, 64, 7, 7])\n",
      "Sequential output shape:\t torch.Size([32, 64, 7, 7])\n",
      "Sequential output shape:\t torch.Size([32, 64, 4, 4])\n",
      "Sequential output shape:\t torch.Size([32, 64, 4, 4])\n",
      "Sequential output shape:\t torch.Size([32, 64, 2, 2])\n",
      "Sequential output shape:\t torch.Size([32, 64, 2, 2])\n",
      "Sequential output shape:\t torch.Size([32, 64, 1, 1])\n",
      "Sequential output shape:\t torch.Size([32, 64, 1, 1])\n",
      "Sequential output shape:\t torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(32, 1, 28, 28)) # sample forward call\n",
    "for layer in thenet:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9d71fa93-50b4-4384-baee-856ced83c02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_epochs = 50\n",
    "num_samples_train = x_train.shape[0]\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "num_samples_valid = x_valid.shape[0]\n",
    "num_batches_valid = num_samples_valid // batch_size\n",
    "num_samples_test = x_test.shape[0]\n",
    "num_batches_test = num_samples_test // batch_size\n",
    "\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "test_acc, test_loss = [], []\n",
    "cur_loss = 0\n",
    "losses = []\n",
    "\n",
    "get_slice = lambda i, size: range(i * size, (i + 1) * size)\n",
    "slce = get_slice(5, batch_size)\n",
    "for i in range(num_batches_train):\n",
    "    slce = get_slice(i, batch_size)\n",
    "    x_batch = x_train[slce]\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "249c82bc-740f-47be-9fdf-1c8b9f6d80b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): singleLayer(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (iden): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "    )\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): singleLayer(\n",
      "      (conv1): ConvTranspose2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (iden): Identity()\n",
      "    )\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    (0): singleLayer(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (iden): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "    )\n",
      "  )\n",
      "  (4): Sequential(\n",
      "    (0): singleLayer(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (iden): Identity()\n",
      "    )\n",
      "  )\n",
      "  (5): Sequential(\n",
      "    (0): singleLayer(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (iden): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "    )\n",
      "  )\n",
      "  (6): Sequential(\n",
      "    (0): singleLayer(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (iden): Identity()\n",
      "    )\n",
      "  )\n",
      "  (7): Sequential(\n",
      "    (0): singleLayer(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (iden): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "    )\n",
      "  )\n",
      "  (8): Sequential(\n",
      "    (0): singleLayer(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (iden): Identity()\n",
      "    )\n",
      "  )\n",
      "  (9): Sequential(\n",
      "    (0): singleLayer(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (iden): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "    )\n",
      "  )\n",
      "  (10): Sequential(\n",
      "    (0): singleLayer(\n",
      "      (conv1): ConvTranspose2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (iden): Identity()\n",
      "    )\n",
      "  )\n",
      "  (11): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(thenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f1cf6cd4-8eb6-494a-a7ba-b0e4f22338ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [10, 64, 1, 1]], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2292/1121286125.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mbatch_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [10, 64, 1, 1]], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "#--> momentum added to SGD optimizer\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9) \n",
    "#--> Adam optimizer with L2 regularization instead of SGD optimizer\n",
    "optimizer = optim.Adam(thenet.parameters(), lr=0.001, weight_decay=1e-4) \n",
    "criterion = nn.HuberLoss()\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('training on', device)\n",
    "thenet.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward -> Backprob -> Update params\n",
    "    ## Train\n",
    "    cur_loss = 0\n",
    "    thenet.train()\n",
    "    for i in range(num_batches_train):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        x_batch = x_train[slce]\n",
    "        x_batch = x_batch.to(device, dtype=torch.float)\n",
    "        \n",
    "        output = thenet(x_batch)\n",
    "        \n",
    "        # compute gradients given loss\n",
    "        target_batch = targets_train[slce]\n",
    "        target_batch = target_batch.to(device, dtype=torch.float)\n",
    "        batch_loss = criterion(output, target_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        batch_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        cur_loss += batch_loss   \n",
    "    losses.append(cur_loss / batch_size)\n",
    "\n",
    "    thenet.eval()\n",
    "    ### Evaluate training\n",
    "    train_preds, train_targs = [], []\n",
    "    for i in range(num_batches_train):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        x_batch = x_train[slce].to(device)\n",
    "        \n",
    "        output = thenet(x_batch)\n",
    "        preds = torch.max(output, 1)[1]\n",
    "        train_targs += list(targets_train[slce])\n",
    "        predscpu = preds.cpu()\n",
    "        train_preds += list(predscpu.data.numpy())\n",
    "    \n",
    "    ### Evaluate validation\n",
    "    val_preds, val_targs = [], []\n",
    "    for i in range(num_batches_valid):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        x_batch = x_valid[slce].to(device)\n",
    "        \n",
    "        output = thenet(x_batch)\n",
    "        preds = torch.max(output, 1)[1]\n",
    "        predscpu = preds.cpu()\n",
    "        val_preds += list(predscpu.data.numpy())\n",
    "        val_targs += list(targets_valid[slce])\n",
    "\n",
    "    train_acc_cur = accuracy_score(train_targs, train_preds)\n",
    "    valid_acc_cur = accuracy_score(val_targs, val_preds)\n",
    "    \n",
    "    train_acc.append(train_acc_cur)\n",
    "    valid_acc.append(valid_acc_cur)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        test_preds, test_targs = [], []\n",
    "        for i in range(num_batches_test):\n",
    "            slce = get_slice(i, batch_size)\n",
    "            x_batch = x_test[slce].to(device)\n",
    "\n",
    "            output = thenet(x_batch)\n",
    "            preds = torch.max(output, 1)[1]\n",
    "            predscpu = preds.cpu()\n",
    "            test_preds += list(predscpu.data.numpy())\n",
    "            test_targs += list(targets_test[slce])\n",
    "        test_acc_cur = accuracy_score(test_targs, test_preds)\n",
    "        test_acc.append(test_acc_cur)\n",
    "        \n",
    "        print(\"Epoch %2i : Train Loss %f , Train acc %f, Valid acc %f, Test acc %f\" % (\n",
    "                epoch+1, losses[-1], train_acc_cur, valid_acc_cur, test_acc_cur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0abecc-72d2-4b19-bb48-9b53d27b08ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
